import os
import sys
import re

from kiwipiepy import Kiwi, TypoTransformer, basic_typos, MorphemeSet, sw_tokenizer
from kiwipiepy.utils import Stopwords

curpath = os.path.dirname(os.path.abspath(__file__))

class FileReader:
    def __init__(self, path):
        self.path = path

    def __iter__(self):
        yield from open(self.path, encoding='utf-8')

def test_glue_empty():
    kiwi = Kiwi()
    kiwi.glue([])

def test_repr():
    kiwi = Kiwi()
    print(repr(kiwi))

def test_morpheme_set():
    kiwi = Kiwi()
    ms = MorphemeSet(kiwi, ["ë¨¹/VV", "ì‚¬ëŒ", ("ê³ ë§™", "VA")])
    print(repr(ms))
    assert len(ms) == 3

def test_blocklist():
    kiwi = Kiwi()
    tokens = kiwi.tokenize("ê³ ë§ˆì›€ì„")
    assert tokens[0].form == "ê³ ë§ˆì›€"
    
    tokens = kiwi.tokenize("ê³ ë§ˆì›€ì„", blocklist=['ê³ ë§ˆì›€'])
    assert tokens[0].form == "ê³ ë§™"

def test_swtokenizer():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    print(tokenizer.vocab)
    print(tokenizer.config)
    strs = [
        "",
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.", 
        "ê°ì‚¬íˆ ë¨¹ê² ìŠµë‹ˆë‹¹!",
        "ë…¸ë˜ì§„ ì†í†±ì„ ë´¤ë˜ê±¸ìš”.",
        "ì œì„ìŠ¤ì›¹ìš°ì£¼ì²œì²´ë§ì›ê²½",
        "ê·¸ë§Œí•´ì—¬~",
    ]
    for s in strs:
        token_ids = tokenizer.encode(s)
        token_ids, offset = tokenizer.encode(s, return_offsets=True)
        decoded = tokenizer.decode(token_ids)
        assert s == decoded

def test_swtokenizer_batch():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    strs = [
        "",
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.", 
        "ê°ì‚¬íˆ ë¨¹ê² ìŠµë‹ˆë‹¹!",
        "ë…¸ë˜ì§„ ì†í†±ì„ ë´¤ë˜ê±¸ìš”.",
        "ì œì„ìŠ¤ì›¹ìš°ì£¼ì²œì²´ë§ì›ê²½",
        "ê·¸ë§Œí•´ì—¬~",
    ]
    for token_ids, s in zip(tokenizer.encode(strs), strs):
        decoded = tokenizer.decode(token_ids)
        assert s == decoded

def test_swtokenizer_morph():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    
    token_ids = tokenizer.encode("í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.")

    morphs = [
        ('í•œêµ­ì–´', 'NNP', False), 
        ('ì—', 'JKB',), 
        ('íŠ¹í™”', 'NNG', True), 
        ('ë˜', 'XSV',), 
        ('á†«', 'ETM', False), 
        ('í† í¬ë‚˜ì´ì €', 'NNG', True), 
        ('ì´', 'VCP', False), 
        ('á†¸ë‹ˆë‹¤', 'EF',), 
        ('.', 'SF', False),
    ]

    token_ids_from_morphs = tokenizer.encode_from_morphs(morphs)

    assert (token_ids == token_ids_from_morphs).all()

    token_ids_from_morphs, offsets = tokenizer.encode_from_morphs(morphs, return_offsets=True)

    assert offsets.tolist() == [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [5, 6], [5, 6], [6, 7], [7, 8], [8, 9]]

def test_swtokenizer_tokenize_encode():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    sents = [
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.",
        "í™ˆí˜ì´ì§€ëŠ” https://bab2min.github.io/kiwipiepy ì…ë‹ˆë‹¤."
    ]
    
    for sent in sents:
        ref_token_ids = tokenizer.encode(sent)
        ref_morphs = tokenizer.kiwi.tokenize(sent, normalize_coda=True, z_coda=True)
        morphs, token_ids, offset = tokenizer.tokenize_encode(sent, return_offsets=True)
        assert [m.tagged_form for m in morphs] == [m.tagged_form for m in ref_morphs]
        assert token_ids.tolist() == ref_token_ids.tolist()

    for (morphs, token_ids, offset), sent in zip(tokenizer.tokenize_encode(sents, return_offsets=True), sents):
        ref_token_ids = tokenizer.encode(sent)
        ref_morphs = tokenizer.kiwi.tokenize(sent, normalize_coda=True, z_coda=True)
        assert [m.tagged_form for m in morphs] == [m.tagged_form for m in ref_morphs]
        assert token_ids.tolist() == ref_token_ids.tolist()

def test_swtokenizer_morph_offset():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/tokenizers/kor.32k.json')
    morphs = [
        ('ì¹¼ìŠ˜', 'NNG', True), 
        ('Â·', 'SP', False), 
        ('ë§ˆê·¸ë„¤ìŠ˜', 'NNG', False), 
        ('ë“±', 'NNB', True), 
        ('ì´', 'JKS', False), 
        ('ë§ì´', 'MAG', True), 
        ('í•¨ìœ ', 'NNG', True), 
        ('ë˜', 'XSV', False), 
        ('ì–´', 'EC', False), 
        ('ìˆ', 'VX', True), 
        ('ì–´', 'EC', False)
    ]
    token_ids, offsets = tokenizer.encode_from_morphs(morphs, return_offsets=True)
    assert len(token_ids) == len(offsets)
    assert offsets[2:7].tolist() == [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3]]

def test_swtokenizer_trainer_empty():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [], 
        config,
        4000,
    )

def test_swtokenizer_trainer_small():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        ["ì´ëŸ° ì €ëŸ° ë¬¸ì¥", "ì´ë ‡ê²Œ ì €ë ‡ê²Œ ì²˜ë¦¬í•´ì„œ", "ì´ë ‡ê³  ì €ë ‡ê³ ", "ì´ëŸ° ì €ëŸ° ê²°ê³¼ë¥¼", "ì–»ì—ˆë‹¤"], 
        config,
        4000,
    )

def test_swtokenizer_trainer_digits():
    kiwi = Kiwi(num_workers=1)
    config = sw_tokenizer.SwTokenizerConfig()

    tokenizer = sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [f"ë“œë””ì–´ ì œ{i}íšŒ í‰ê°€" for i in range(1, 1000)], 
        config,
        4000,
        kiwi=kiwi,
        prevent_mixed_digit_tokens=False,
    )
    mixed_digit = [k for k in tokenizer.vocab if re.search(r'ì œ[0-9]|[0-9]íšŒ', k)]
    assert len(mixed_digit) > 0

    tokenizer = sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [f"ë“œë””ì–´ ì œ{i}íšŒ í‰ê°€" for i in range(1, 1000)], 
        config,
        4000,
        kiwi=kiwi,
        prevent_mixed_digit_tokens=True,
    )
    mixed_digit = [k for k in tokenizer.vocab if re.search(r'ì œ[0-9]|[0-9]íšŒ', k)]
    assert len(mixed_digit) == 0

def test_swtokenizer_trainer():
    import itertools

    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        itertools.chain.from_iterable(open(f, encoding='utf-8') for f in (
            'kiwipiepy/documentation.md', 
            'kiwipiepy/_wrap.py', 
        )), 
        config,
        4000,
    )

def test_swtokenizer_trainer_multiple_vocab_sizes():
    import itertools

    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        ['test.json', 'test2.json', 'test3.json'], 
        itertools.chain.from_iterable(open(f, encoding='utf-8') for f in (
            'kiwipiepy/documentation.md', 
            'kiwipiepy/_wrap.py', 
        )), 
        config,
        [4000, 2000, 1000],
    )

def test_analyze_single():
    kiwi = Kiwi()
    for line in open(curpath + '/test_corpus/constitution.txt', encoding='utf-8'):
        toks, score = kiwi.analyze(line)[0]
    for t in toks:
        print(t.form, t.tag, t.start, t.end, t.len, t.id, t.base_form, t.base_id)
        break

def test_extract_words():
    kiwi = Kiwi()
    ret = kiwi.extract_words(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2)
    print(ret)

def test_perform():
    kiwi = Kiwi()
    for res in kiwi.perform(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2):
        print(res)
        break

def test_tweet():
    kiwi = Kiwi()
    kiwi.analyze('''#ë°”ë‘‘#ì¥ê¸°#ì˜¤ëª© ê·€ìš”ë¯¸#ë³´ë“œíŒğŸ¥
#ì–´ë¦°ì´ì„ë¸”ë¦¬ì˜ ë†€ì´ì˜€ëŠ”ë°, ì´ì œëŠ” ê°€ë¬¼ê°¸ë¬¼ğŸ™„ëª¨ë¥´ê² 
ì¥ì´ìš”~ë©ì´ìš”~ã…ã…ã…ë‹¤ì‹œ í•œ ë²ˆ ì¬ë¯¸ë¥¼ ë¶™ì—¬ ë³´ê¹Œã…
í•  ì¼ì´ íƒœì‚°ì¸ë°ğŸ˜­, í•˜ê³  ì‹¶ì€ê±´ ë¬´ê¶ë¬´ì§„ğŸ¤¦â€â™€ï¸ í° ì¼ì´ë‹¤''')

def test_new_analyze_multi():
    kiwi = Kiwi()
    for res in kiwi.analyze(open(curpath + '/test_corpus/constitution.txt', encoding='utf-8')):
        pass

def test_bug_33():
    kiwi = Kiwi()
    kiwi.add_user_word('ê¹€ê°‘ê°‘', 'NNP')
    print(kiwi.analyze("ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘"))

def test_bug_38():
    text = "ì´ ì˜ˆìœ ê½ƒì€ ë…ì„ í’ˆì—ˆì§€ë§Œ ì§„ì§œ ì•„ë¦„ë‹¤ì›€ì„ ê°€ì§€ê³  ìˆì–´ìš”"
    kiwi = Kiwi(integrate_allomorph=True)
    print(kiwi.analyze(text))
    kiwi = Kiwi(integrate_allomorph=False)
    print(kiwi.analyze(text))

def test_property():
    kiwi = Kiwi()
    print(kiwi.integrate_allomorph)
    kiwi.integrate_allomorph = False
    print(kiwi.integrate_allomorph)
    print(kiwi.cutoff_threshold)
    kiwi.cutoff_threshold = 1
    print(kiwi.cutoff_threshold)

def test_stopwords():
    kiwi = Kiwi()
    tokens, _ = kiwi.analyze('ë¶ˆìš©ì–´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤ '
                             'ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ ìŸ¤ë„ ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ '
                             'ì§€ê¸ˆì€ 2021ë…„ 11ì›”ì´ë‹¤.')[0]
    stopwords = Stopwords()
    print(set(tokens) - set(stopwords.filter(tokens)))
    filename = curpath + '/test_corpus/custom_stopwords.txt'
    stopwords = Stopwords(filename)

    stopwords.add(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == True

    stopwords.remove(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == False
    print(set(tokens) - set(stopwords.filter(tokens)))

def test_tokenize():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    tokens = kiwi.tokenize(text, normalize_coda=True)
    print(tokens)

    tokens_by_sent = kiwi.tokenize(text, normalize_coda=True, split_sents=True)
    for tokens in tokens_by_sent:
        print(tokens)

def test_tokenize_with_stopwords():
    kiwi = Kiwi()
    stopwords = Stopwords()
    tokens = kiwi.tokenize("[^^ ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤.]", stopwords=stopwords)

    assert tokens[0].form == 'ê°•ì•„ì§€'
    assert tokens[1].form == 'ì¢‹ì•„í•˜'

def test_split_into_sents():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    sents = kiwi.split_into_sents(text, normalize_coda=True)
    assert len(sents) == 6

    assert sents[0].text == "ë‹¤ë…€ì˜¨ í›„ê¸°"
    assert sents[1].text == "<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.>"
    assert sents[2].text == "ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš”"
    assert sents[3].text == "ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã…"
    assert sents[4].text == "ê·¸ ë§›ì´ í¬ìœ¼.."
    assert sents[5].text == "ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"

def test_add_rule():
    kiwi = Kiwi(load_typo_dict=False)
    ores, oscore = kiwi.analyze("í–ˆì–´ìš”! í•˜ì–ì•„ìš”! í• ê¹Œìš”? ì¢‹ì•„ìš”!")[0]

    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=0)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©? ì¢‹ì•„ìš©!")[0]
    assert score == oscore

    kiwi = Kiwi(load_typo_dict=False)
    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=-1)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©? ì¢‹ì•„ìš©!")[0]
    assert score == oscore - 4

def test_add_pre_analyzed_word():
    kiwi = Kiwi()
    ores = kiwi.tokenize("íŒ…ê²¼ì–´")

    try:
        kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV"), "ì—ˆ/EP", "ì–´/EF"])
        raise AssertionError("expected to raise `ValueError`")
    except ValueError:
        pass
    except:
        raise

    kiwi.add_user_word("íŒ…ê¸°", "VV", orig_word="íŠ•ê¸°")
    kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV", 0, 2), ("ì—ˆ", "EP", 1, 2), ("ì–´", "EF", 2, 3)])

    res = kiwi.tokenize("íŒ…ê²¼ì–´...")

    assert res[0].form == "íŒ…ê¸°" and res[0].tag == "VV" and res[0].start == 0 and res[0].end == 2
    assert res[1].form == "ì—ˆ" and res[1].tag == "EP" and res[1].start == 1 and res[1].end == 2
    assert res[2].form == "ì–´" and res[2].tag == "EF" and res[2].start == 2 and res[2].end == 3
    assert res[3].form == "..." and res[3].tag == "SF" and res[3].start == 3 and res[3].end == 6

def test_space_tolerance():
    kiwi = Kiwi()
    s = "ë„ ì–´ ì“° ê¸° ë¬¸ ì œ ê°€ ìˆ ìŠµ ë‹ˆ ë‹¤"
    kiwi.space_tolerance = 0
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 1
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 2
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 3
    print(kiwi.tokenize(s))

def test_space():
    kiwi = Kiwi()
    res0 = kiwi.space("ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.")
    assert res0 == "ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë„¤ ì´ê±¸ êµì •í•´ ì¤˜."

    res1 = kiwi.space(" ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ")
    assert res1 == " ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤, ì´ê±¸ êµì •í•´ ì¤˜. "

    res2 = kiwi.space("<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.")
    assert res2 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res3 = kiwi.space("<Kiwipiepy>ëŠ” í˜• íƒœ ì†Œ ë¶„ ì„ ê¸° ì´ ì— ìš”~ 0.11.0 ë²„ ì „ ì´ ë‚˜ ì™” ì–´ ìš” .", reset_whitespace=True)
    assert res3 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res_a = list(kiwi.space([
        "ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.",
        " ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ",
        "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.",
    ]))
    assert res_a == [res0, res1, res2]

def test_glue():
    chunks = """KorQuAD 2.0ì€ ì´ 100,000+ ìŒìœ¼ë¡œ êµ¬ì„±ëœ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì´ë‹¤. ê¸°ì¡´ ì§ˆì˜ì‘ë‹µ í‘œì¤€ ë°ì´
í„°ì¸ KorQuAD 1.0ê³¼ì˜ ì°¨ì´ì ì€ í¬ê²Œ ì„¸ê°€ì§€ê°€ ìˆëŠ”ë° ì²« ë²ˆì§¸ëŠ” ì£¼ì–´ì§€ëŠ” ì§€ë¬¸ì´ í•œë‘ ë¬¸ë‹¨ì´ ì•„ë‹Œ ìœ„
í‚¤ë°±ê³¼ í•œ í˜ì´ì§€ ì „ì²´ë¼ëŠ” ì ì´ë‹¤. ë‘ ë²ˆì§¸ë¡œ ì§€ë¬¸ì— í‘œì™€ ë¦¬ìŠ¤íŠ¸ë„ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— HTML tagë¡œ
êµ¬ì¡°í™”ëœ ë¬¸ì„œì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•˜ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹µë³€ì´ ë‹¨ì–´ í˜¹ì€ êµ¬ì˜ ë‹¨ìœ„ë¿ ì•„ë‹ˆë¼ ë¬¸ë‹¨, í‘œ, ë¦¬
ìŠ¤íŠ¸ ì „ì²´ë¥¼ í¬ê´„í•˜ëŠ” ê¸´ ì˜ì—­ì´ ë  ìˆ˜ ìˆë‹¤. Baseline ëª¨ë¸ë¡œ êµ¬ê¸€ì´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•œ BERT
Multilingualì„ í™œìš©í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼ F1 ìŠ¤ì½”ì–´ 46.0%ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ëŠ” ì‚¬ëŒì˜ F1 ì ìˆ˜
85.7%ì— ë¹„í•´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¡œ, ë³¸ ë°ì´í„°ê°€ ë„ì „ì ì¸ ê³¼ì œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë³¸ ë°ì´í„°ì˜ ê³µê°œë¥¼ í†µí•´
í‰ë¬¸ì— êµ­í•œë˜ì–´ ìˆë˜ ì§ˆì˜ì‘ë‹µì˜ ëŒ€ìƒì„ ë‹¤ì–‘í•œ ê¸¸ì´ì™€ í˜•ì‹ì„ ê°€ì§„ real world taskë¡œ í™•ì¥í•˜ê³ ì í•œë‹¤""".split('\n')
    
    kiwi = Kiwi()
    ret, space_insertions = kiwi.glue(chunks, return_space_insertions=True)
    assert space_insertions == [False, False, True, False, True, True, True]

def test_join():
    kiwi = Kiwi()
    tokens = kiwi.tokenize("ì´ë ‡ê²Œ í˜•íƒœì†Œë¡œ ë¶„í•´ëœ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹  ìˆ˜ ìˆì„ê¹Œìš”?")
    
    assert kiwi.join(tokens) == "ì´ë ‡ê²Œ í˜•íƒœì†Œë¡œ ë¶„í•´ëœ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹  ìˆ˜ ìˆì„ê¹Œìš”?"

    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV"), ("ì–´ìš”", "EF")]) 
        == "ì™œ ì €í•œí…Œ ë¬¼ì–´ìš”"
    )
    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV-R"), ("ì–´ìš”", "EF")])
        == "ì™œ ì €í•œí…Œ ë¬»ì–´ìš”"
    )
    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV-I"), ("ì–´ìš”", "EF")])
        == "ì™œ ì €í•œí…Œ ë¬¼ì–´ìš”"
    )

def test_bug_87():
    text = "í•œê¸€(éŸ“ã[1], ì˜ì–´: Hangeul[2]ë˜ëŠ” Hangul[3])ì€ í•œêµ­ì–´ì˜ ê³µì‹ë¬¸ìë¡œì„œ, ì„¸ì¢…ì´ í•œêµ­ì–´ë¥¼ í‘œê¸°í•˜ê¸° ìœ„í•˜ì—¬ ì°½ì œí•œ ë¬¸ìì¸ 'í›ˆë¯¼ì •ìŒ'(è¨“æ°‘æ­£éŸ³)ì„ 20ì„¸ê¸° ì´ˆë°˜ ì´í›„ ë‹¬ë¦¬ ì´ë¥´ëŠ” ëª…ì¹­ì´ë‹¤.[4][5] í•œê¸€ì´ë€ ì´ë¦„ì€ ì£¼ì‹œê²½ ì„ ìƒê³¼ êµ­ì–´ì—°êµ¬í•™íšŒ íšŒì›ë“¤ì— ì˜í•´ ì§€ì–´ì§„ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë©°[6][7][8][9] ê·¸ ëœ»ì€ 'ìœ¼ëœ¸ì´ ë˜ëŠ” í°ê¸€', 'ì˜¤ì§ í•˜ë‚˜ë¿ì¸ í°ê¸€', 'í•œêµ­ì¸ì˜ ê¸€ì'ì´ë‹¤.[6][10] í•œê¸€ì˜ ë˜ ë‹¤ë¥¸ ë³„ì¹­ìœ¼ë¡œëŠ” ì •ìŒ(æ­£éŸ³), ì–¸ë¬¸(è«ºæ–‡)[11], ì–¸ì„œ(è«ºæ›¸), ë°˜ì ˆ(ååˆ‡), ì•”í´, ì•„í–‡ê¸€, ê°€ê°¸ê¸€, êµ­ë¬¸(åœ‹æ–‡)[12] ë“±ì´ ìˆë‹¤.[5]"
    kiwi = Kiwi()
    print(kiwi.join(kiwi.tokenize(text)))

def test_typo_transformer():
    print(basic_typos.generate("ì•ˆë¼"))

def test_typo_correction():
    if sys.maxsize <= 2**32:
        print("[skipped this test in 32bit OS.]", file=sys.stderr)
        return
    kiwi = Kiwi(typos='basic')
    ret = kiwi.tokenize("ì™¸ì•Šë€ëŒ€?")
    assert ret[0].form == 'ì™œ'
    assert ret[1].form == 'ì•ˆ'
    assert ret[2].form == 'ë˜'
    assert ret[3].form == 'á†«ëŒ€'
    print(ret)

def test_sbg():
    kiwi = Kiwi(model_type='knlm')
    print(kiwi.tokenize('ì´ ë²ˆí˜¸ë¡œ ì „í™”ë¥¼ ì´ë”°ê°€ ê¼­ ë°˜ë“œì‹œ ê±¸ì–´.'))
    kiwi = Kiwi(model_type='sbg')
    print(kiwi.tokenize('ì´ ë²ˆí˜¸ë¡œ ì „í™”ë¥¼ ì´ë”°ê°€ ê¼­ ë°˜ë“œì‹œ ê±¸ì–´.'))

def test_issue_92():
    if sys.maxsize <= 2**32:
        print("[skipped this test in 32bit OS.]", file=sys.stderr)
        return
    kiwi = Kiwi(typos='basic')
    try:
        kiwi.join(kiwi.analyze('ì˜'))
        raise AssertionError("expected to raise `ValueError`")
    except ValueError:
        pass
    except:
        raise

    kiwi.join([('ì‚¬ë‘','NNG')])

def test_unicode():
    kiwi = Kiwi()
    print(repr(kiwi.tokenize("ê²° ë¡  189   ì°¸ê³ ë¬¸í—Œ 191   ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ")))
