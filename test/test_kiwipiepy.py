import os
import sys
import re
import tempfile
import itertools

from kiwipiepy import Kiwi, TypoTransformer, basic_typos, MorphemeSet, sw_tokenizer, PretokenizedToken, extract_substrings
from kiwipiepy.utils import Stopwords

curpath = os.path.dirname(os.path.abspath(__file__))

class FileReader:
    def __init__(self, path):
        self.path = path

    def __iter__(self):
        yield from open(self.path, encoding='utf-8')

def test_extract_substrings():
    s = ("ì, ë„ˆ ì˜¤ëŠ˜ í•˜ë£¨ ë­ í–ˆë‹ˆ? "
		"ë‚œ ì˜¤ëŠ˜ í•˜ë£¨ê°€ ì¢€ ë‹¨ìˆœí–ˆì§€. ìŒ ë­ í–ˆëŠ”ë°? "
		"ì•„ì¹¨ì— ìˆ˜ì—… ë°›ë‹¤ê°€ ì˜¤êµ¬ ê·¸~ í•™ìƒì´, ì‘. ë¯¸ì°Œ í•™ìƒì¸ë°, "
		"ìŒ. ë˜ê²Œ ê·€ì—½ê²Œ ìƒê²¼ë‹¤, ìŒ. ë˜ê²Œ ì›ƒìœ¼ë©´ ê³°ëŒì´ ì¸í˜•ê°™ì´ ìƒê²¼ì–´. "
		"ê³°ëŒì´? ì‘ ì•„ë‹ˆ ê³°ëŒì´ë„ ì•„ë‹ˆêµ¬, ì–´ì¨Œë“  ë¬´ìŠ¨ ì¸í˜•ê°™ì´ ìƒê²¼ì–´, "
		"íœë”ê³° ê·¸ëŸ° ê±°, ì™œ ì´ë ‡ê²Œ ë‹®ì€ ì‚¬ëŒì„ ëŒ€ ë´. "
		"ë‚´ê°€ ì•„ëŠ” ì‚¬ëŒ ì¤‘ì—ì„œ, í•œë¬´ í•œë¬´? ê¸ˆë¶•ì–´ì–ì•„? "
		"ë§ì–´ ëˆˆë„ ì´ë ‡ê²Œ í†¡ íŠ€ì–´ë‚˜ì˜¤êµ¬, ì–´. í•œë¬´? ì¡°ê¸ˆ ì˜ ìƒê¸´ í•œë¬´. "
		"ì˜ ìƒê¸´ ê²Œ ì•„ë‹ˆë¼ ê·€ì—¬ìš´ í•œë¬´. ê·€ì—¬ìš´ í•œë¬´? "
		"ì–´ í•™ì›ì—ì„œ ë³„ëª…ë„ ê·€ì—¬ìš´ í•œë¬´ì˜€ì–´. "
		"ì‘. ëˆˆì´ ë˜¥ê·¸ë˜ ê°€ì§€ê³  ê·¸ë˜? ì–´. ì¢€ íŠ¹ì´í•œ ì‚¬ëŒì´êµ¬ë‚˜.")
    substrings = extract_substrings(s, min_cnt=2, min_length=2, longest_only=True, stop_chr=' ')
    print(substrings)
    assert len(substrings) == 23

def test_false_positive_sb():
    kiwi = Kiwi()
    for s in [
        "í•˜ë‹¤. ê°€ìš´ë° ë¹„ë‹ì„ ìš”ë ‡ê²Œ ë²—ê²¨ì£¼ê³ ìš”!",
		"ì, ì´ê²ƒì´ `ì—´ì‡ `ë‹¤.`` ì•”ìƒì¸ ì•ì˜ ìº¡ìŠì´ ì—´ë¦¬ë©° ê·¸ê³³ì—ì„œ ìƒˆë¡œìš´ íŒŒì›Œì—… ì•„ì´í…œ `ì¿ ë‚˜ì´`ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.",
		"ê¸°ê³„ëŠ” ëª…ë ¹ë§Œ ë“£ëŠ”ë‹¤.ë¼ëŠ” ìƒê°ì´ ì´ì œ ì‚¬ëŒë“¤ì—ê²Œ ì™„ì „íˆ ì •ì°©ì´ ë˜ì—ˆë‹¤ëŠ” ìƒí™©ì¸ë°, ê·¸ëŸ´ì‹¸í•˜ì£ ",
		"í›„ë°˜ ë¹¨ê°„ ëª¨ì•„ì´ë“¤ì˜ ê³µê²©ì€ ì—„ì²­ë‚˜ê²Œ ê±°ì„¸ë‹¤.ìƒí•˜ë¡œ ì í”„í•˜ë©° ì´ì˜¨ë§ì„ ë°œì‚¬í•˜ëŠ” ì¤‘ë³´ìŠ¤ ëª¨ì•„ì´ìƒë“¤.",
		"ë˜ ì „í™”ì„¸, ì „ê¸°ì„¸, ë³´í—˜ë£Œë“± ì›” ì •ê¸°ì§€ì¶œë„ ì§€ì¶œí†µì¥ìœ¼ë¡œ ë°”ê¾¼ ë‹¤. ì…‹ì§¸, ë¬¼ê±´ì„ ì‚´ë• ë¬´ì¡°ê±´ ì¹´ë“œë¡œ ê¸ëŠ”ë‹¤.",
		"ì—í‹°í•˜ë“œí•­ê³µì´ ìµœê³ ì˜ ì´ì½”ë…¸ë¯¸ í´ë˜ìŠ¤ ìƒì„ ë‘ ë²ˆì§¸ë¡œ ë°›ì€ í•´ëŠ” 2020ë…„ì´ë‹¤. ì´ì „ì—ëŠ” 2012ë…„ê³¼ 2013ë…„ì— ìµœê³ ì˜ ì´ì½”ë…¸ë¯¸ í´ë˜ìŠ¤ ìƒì„ ìˆ˜ìƒí•œ ì ì´ ìˆì–´ìš”.",
		"ì€ë¯¸í¬ëŠ” ìƒˆë¡œìš´ ì‚¶ì„ ì‹œì‘í•˜ê¸°ë¡œ ê²°ì‹¬í•œ í›„ 6ë…„ê°„ì˜ ìŠµì‘ê¸°ê°„ì„ ê±°ì³ 1996ë…„ ë‹¨í¸ ã€Šëˆ„ì—ëŠ” ê³ ì¹˜ ì†ì—ì„œ ë¬´ìŠ¨ ê¿ˆì„ ê¾¸ëŠ”ê°€ã€‹ë¡œ ì „ë‚¨ì¼ë³´ ì‹ ì¶˜ë¬¸ì˜ˆ ë‹¹ì„ ëë‹¤. ë”°ë¼ì„œ ì€ë¯¸í¬ê°€ ã€Šëˆ„ì—ëŠ” ê³ ì¹˜ ì†ì—ì„œ ë¬´ìŠ¨ ê¿ˆì„ ê¾¸ëŠ”ê°€ã€‹ë¡œ ìƒì„ ë°›ê¸° ì „ì— ì—°ìŠµ ì‚¼ì•„ ì†Œì„¤ì„ ì§‘í•„í–ˆë˜ ê¸°ê°„ì€ 6ë…„ì´ë‹¤.",
		"ë„ì„œì „ì—ì„œ ê´€ëŒê°ì˜ ê´€ì‹¬ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” í”„ë¡œê·¸ë¨ìœ¼ë¡œëŠ” 'ì¸ë¬¸í•™ ì•„ì¹´ë°ë¯¸'ê°€ ìˆì–´ìš”. ì´ í”„ë¡œê·¸ë¨ì—ì„œëŠ” ìœ ì‹œë¯¼ ì „ ì˜ì›, ê´‘ê³ ì¸ ë°•ì›…í˜„ ì”¨ ë“±ì´ ë¬¸í™” ì—­ì‚¬ ë¯¸í•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ëŒ€í•´ ê°•ì˜í•  ì˜ˆì •ì´ë‹¤. ë˜í•œ, 'ë¶ ë©˜í†  í”„ë¡œê·¸ë¨'ë„ ì´ì–´ì ¸ìš”. ì´ í”„ë¡œê·¸ë¨ì—ì„œëŠ” ê° ë¶„ì•¼ ì „ë¬¸ê°€ë“¤ì´ ê²½í—˜ê³¼ ë…¸í•˜ìš°ë¥¼ ì „ìˆ˜í•´ ì£¼ëŠ” í”„ë¡œê·¸ë¨ìœ¼ë¡œ, ì‹œ ì°½ì‘(ì´ì •ë¡ ì‹œì¸), ë²ˆì—­(ê°•ì£¼í—Œ ë²ˆì—­ê°€), ë¶ ë””ìì¸(ì˜¤ì§„ê²½ ë¶ë””ìì´ë„ˆ) ë“±ì˜ ë¶„ì•¼ì—ì„œ ë©˜í† ë§ì´ ì´ë¤„ì ¸ìš”.",
    ]:
        assert all(t.tag != 'SB' for t in kiwi.tokenize(s))

def test_repr():
    kiwi = Kiwi()
    print(repr(kiwi))

def test_morpheme_set():
    kiwi = Kiwi()
    ms = MorphemeSet(kiwi, ["ë¨¹/VV", "ì‚¬ëŒ", ("ê³ ë§™", "VA")])
    print(repr(ms))
    assert len(ms) == 3

def test_load_user_dictionary():
    kiwi = Kiwi()
    try:
        raised = False
        kiwi.load_user_dictionary('non-existing-file.txt')
    except OSError as e:
        raised = True
        print(e)
    finally:
        assert raised

    with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as f:
        f.write('ì˜ëª»ëœ í¬ë§·ì˜ íŒŒì¼ì…ë‹ˆë‹¤\n')
    try:
        raised = False
        kiwi.load_user_dictionary(f.name)
    except ValueError as e:
        raised = True
        print(e)
    finally:
        assert raised

def test_issue_158():
    print(len(Kiwi().tokenize('ë³´í†µ' * 40000)))

def test_list_all_scripts():
    kiwi = Kiwi()
    print(kiwi.list_all_scripts())

def test_blocklist():
    kiwi = Kiwi()
    tokens = kiwi.tokenize("ê³ ë§ˆì›€ì„")
    assert tokens[0].form == "ê³ ë§ˆì›€"
    
    tokens = kiwi.tokenize("ê³ ë§ˆì›€ì„", blocklist=['ê³ ë§ˆì›€'])
    assert tokens[0].form == "ê³ ë§™"

def test_pretokenized():
    kiwi = Kiwi(load_multi_dict=False)
    text = "ë“œë””ì–´íŒ¨íŠ¸ì™€ ë§¤íŠ¸ê°€ 2017ë…„ì— êµ­ë‚´ ê°œë´‰í–ˆë‹¤. íŒ¨íŠ¸ì™€ë§¤íŠ¸ëŠ” 2016ë…„..."

    res = kiwi.tokenize(text, pretokenized=[
        (3, 9),
        (11, 16),
        (34, 39)
    ])
    assert res[1].form == "íŒ¨íŠ¸ì™€ ë§¤íŠ¸"
    assert res[1].tag == "NNP"
    assert res[3].form == "2017ë…„"
    assert res[3].tag == "NNP"
    assert res[13].form == "2016ë…„"
    assert res[13].tag == "NNP"

    res = kiwi.tokenize(text, pretokenized=[
        (3, 9),
        (11, 16, 'NNG'),
        (34, 39, 'NNG')
    ])
    assert res[3].form == "2017ë…„"
    assert res[3].tag == "NNG"
    assert res[13].form == "2016ë…„"
    assert res[13].tag == "NNG"

    res = kiwi.tokenize(text, pretokenized=[
        (27, 29, PretokenizedToken('í˜íŠ¸', 'NNB', 0, 2)),
        (30, 32),
        (21, 24, [PretokenizedToken('ê°œë´‰í•˜', 'VV', 0, 3), PretokenizedToken('ì—ˆ', 'EP', 2, 3)])
    ])
    assert res[7].form == "ê°œë´‰í•˜"
    assert res[7].tag == 'VV'
    assert res[7].start == 21
    assert res[7].len == 3
    assert res[8].form == "ì—ˆ"
    assert res[8].tag == 'EP'
    assert res[8].start == 23
    assert res[8].len == 1
    assert res[11].form == "í˜íŠ¸"
    assert res[11].tag == 'NNB'
    assert res[13].form == "ë§¤íŠ¸"
    assert res[13].tag == 'NNG'

    res = kiwi.tokenize(text, pretokenized=lambda x:[i.span() for i in re.finditer(r'íŒ¨íŠ¸ì™€ ?ë§¤íŠ¸', x)])
    assert res[1].form == "íŒ¨íŠ¸ì™€ ë§¤íŠ¸"
    assert res[1].span == (3, 9)
    assert res[12].form == "íŒ¨íŠ¸ì™€ë§¤íŠ¸"
    assert res[12].span == (27, 32)

    res = kiwi.tokenize(text, pretokenized=[
        (3, 5, PretokenizedToken('í˜íŠ¸', 'NNB', 0, 2)),
    ])
    assert res[1].form == 'í˜íŠ¸'

    try:
        is_raised = False
        kiwi.tokenize("some text", pretokenized=[(-1, 0)])
    except ValueError as e:
        is_raised = True
        print(e)
    finally:
        assert is_raised

    try:
        is_raised = False
        kiwi.tokenize("some text", pretokenized=[(0, 1000)])
    except ValueError as e:
        is_raised = True
        print(e)
    finally:
        assert is_raised

    try:
        is_raised = False
        kiwi.tokenize("some text", pretokenized=[(1, 0)])
    except ValueError as e:
        is_raised = True
        print(e)
    finally:
        assert is_raised

def test_re_word():
    text = '{í‰ë§Œê²½(å¹³æ»¿æ™¯)}ì´ ì‚¬ëŒì„ ì‹œì¼œ {ì¹¨í–¥(æ²ˆé¦™)} 10ëƒ¥ì­ì„ ë°”ì³¤ìœ¼ë¯€ë¡œ'

    kiwi = Kiwi()
    res = kiwi.tokenize(text)

    kiwi.add_re_word(r'\{[^}]+\}', 'NNP')

    res = kiwi.tokenize(text)
    assert res[0].form == '{í‰ë§Œê²½(å¹³æ»¿æ™¯)}'
    assert res[0].tag == 'NNP'
    assert res[0].span == (0, 10)
    assert res[1].tag == 'JKS'
    assert res[6].form == '{ì¹¨í–¥(æ²ˆé¦™)}'
    assert res[6].tag == 'NNP'
    assert res[6].span == (19, 27)

    kiwi.clear_re_words()
    kiwi.add_re_word(r'(?<=\{)([^}]+)(?=\})', lambda m:PretokenizedToken(m.group(1), 'NNP', m.span(1)[0] - m.span(0)[0], m.span(1)[1] - m.span(0)[0]))

    res = kiwi.tokenize(text)
    assert res[1].form == 'í‰ë§Œê²½(å¹³æ»¿æ™¯)'
    assert res[1].tag == 'NNP'
    assert res[1].span == (1, 9)
    assert res[9].form == 'ì¹¨í–¥(æ²ˆé¦™)'
    assert res[9].tag == 'NNP'
    assert res[9].span == (20, 26)

    kiwi.clear_re_words()
    kiwi.add_re_word(r'\{([^}]+)\}', lambda m:PretokenizedToken(m.group(1), 'NNP', m.span(1)[0] - m.span(0)[0], m.span(1)[1] - m.span(0)[0]))

    res = kiwi.tokenize(text)
    assert res[0].form == 'í‰ë§Œê²½(å¹³æ»¿æ™¯)'
    assert res[0].tag == 'NNP'
    assert res[0].span == (0, 10)
    assert res[6].form == 'ì¹¨í–¥(æ²ˆé¦™)'
    assert res[6].tag == 'NNP'
    assert res[6].span == (19, 27)

    res = kiwi.tokenize(text, pretokenized=[(28, 32)])
    assert res[7].form == '10ëƒ¥ì­'
    assert res[7].tag == 'NNP'
    assert res[7].span == (28, 32)

    res = kiwi.tokenize(text, pretokenized=[(1, 4)])

def test_user_value():
    kiwi = Kiwi()
    kiwi.add_user_word('ì‚¬ìš©ìë‹¨ì–´', user_value=['user_value'])
    kiwi.add_user_word('íƒœê·¸', user_value={'tag':'USER_TAG'})
    kiwi.add_re_rule('NNG', 'ë°”ë³´', 'ë°¥ì˜¤', user_value='babo')
    kiwi.add_re_word(r'\{[^}]+\}', 'USER0', user_value={'tag':'SPECIAL'})

    tokens = kiwi.tokenize('ì‚¬ìš©ìë‹¨ì–´ì…ë‹ˆë‹¤.')
    assert tokens[0].form == 'ì‚¬ìš©ìë‹¨ì–´'
    assert tokens[0].user_value == ['user_value']
    assert tokens[1].user_value == None
    assert tokens[2].user_value == None

    tokens = kiwi.tokenize('ì‚¬ìš©ì íƒœê·¸ì´ë‹¤!')
    assert tokens[0].tag == 'NNG'
    assert tokens[1].form == 'íƒœê·¸'
    assert tokens[1].tag == 'USER_TAG'
    assert tokens[1].form_tag == ('íƒœê·¸', 'USER_TAG')
    assert tokens[1].tagged_form == 'íƒœê·¸/USER_TAG'

    tokens = kiwi.tokenize('ë°¥ì˜¤..')
    assert tokens[0].form == 'ë°¥ì˜¤'
    assert tokens[0].tag == 'NNG'
    assert tokens[0].user_value == 'babo'

    tokens = kiwi.tokenize('ì´ë ‡ê²Œ {ì´ê²ƒ}ì€ íŠ¹ë³„í•˜ë‹¤')
    assert tokens[1].form == '{ì´ê²ƒ}'
    assert tokens[1].tag == 'SPECIAL'
    assert tokens[1].user_value == {'tag':'SPECIAL'}
    assert sum(1 for t in tokens if t.user_value is not None) == 1

    tokens = next(kiwi.tokenize(['{ì´ê²ƒ}ì€ íŠ¹ë³„í•˜ë‹¤']))
    assert tokens[0].form == '{ì´ê²ƒ}'
    assert tokens[0].tag == 'SPECIAL'
    assert tokens[0].user_value == {'tag':'SPECIAL'}
    assert sum(1 for t in tokens if t.user_value is not None) == 1

def test_user_value_issue168():
    kiwi = Kiwi()
    text = """ë§ˆí¬ë‹¤ìš´ ì½”ë“œê°€ ì„ì¸ ë¬¸ìì—´
```python
import kiwipiepy
```
ì…ë‹ˆë‹¤."""

    pat1 = re.compile(r'^```python\n.*?^```', flags=re.DOTALL | re.MULTILINE)
    pat2 = re.compile(r'ì…ë‹ˆë‹¤')

    kiwi.add_re_word(pat1, 'USER1', {'tag':'CODE1'})
    kiwi.add_re_word(pat2, 'USER2', {'tag':'CODE2'})
    tokens = kiwi.tokenize(text)
    assert tokens[-3].tag == 'CODE1'
    assert tokens[-2].tag == 'CODE2'

def test_words_with_space():
    kiwi = Kiwi()
    
    assert kiwi.add_user_word('ëŒ€í•™ìƒ ì„ êµíšŒ', 'NNP')
    res1 = kiwi.tokenize('ëŒ€í•™ìƒ ì„ êµíšŒ')
    res2 = kiwi.tokenize('ëŒ€í•™ìƒì„ êµíšŒ')
    res3 = kiwi.tokenize('ëŒ€í•™ìƒ \t ì„ êµíšŒ')
    res4 = kiwi.tokenize('ëŒ€ í•™ìƒì„ êµíšŒ')
    res5 = kiwi.tokenize('ëŒ€ í•™ìƒ ì„ êµíšŒ')
    res6 = kiwi.tokenize('ëŒ€í•™ ìƒì„  êµíšŒ')
    assert len(res1) == 1
    assert len(res2) == 1
    assert len(res3) == 1
    assert len(res4) != 1
    assert len(res5) != 1
    assert len(res6) != 1
    assert res1[0].form == 'ëŒ€í•™ìƒ ì„ êµíšŒ'
    assert res2[0].form == 'ëŒ€í•™ìƒ ì„ êµíšŒ'
    assert res3[0].form == 'ëŒ€í•™ìƒ ì„ êµíšŒ'
    assert res4[0].form != 'ëŒ€í•™ìƒ ì„ êµíšŒ'
    assert res5[0].form != 'ëŒ€í•™ìƒ ì„ êµíšŒ'
    assert res6[0].form != 'ëŒ€í•™ìƒ ì„ êµíšŒ'

    kiwi.space_tolerance = 1
    res1 = kiwi.tokenize('ëŒ€í•™ìƒ ì„ êµíšŒ')
    res2 = kiwi.tokenize('ëŒ€í•™ìƒì„ êµíšŒ')
    res3 = kiwi.tokenize('ëŒ€í•™ìƒ \t ì„ êµíšŒ')
    res4 = kiwi.tokenize('ëŒ€ í•™ìƒì„ êµíšŒ')
    res5 = kiwi.tokenize('ëŒ€ í•™ìƒ ì„ êµíšŒ')
    res6 = kiwi.tokenize('ëŒ€í•™ ìƒì„  êµíšŒ')
    assert len(res1) == 1
    assert len(res2) == 1
    assert len(res3) == 1
    assert len(res4) == 1
    assert len(res5) == 1
    assert len(res6) != 1

    kiwi.space_tolerance = 0
    assert kiwi.add_user_word('ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥', 'NNP')
    res1 = kiwi.tokenize('ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥')
    res2 = kiwi.tokenize('ë†í˜‘ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥')
    res3 = kiwi.tokenize('ë†í˜‘ ìš©ì¸ìœ¡ê°€ê³µ ê³µì¥')
    res4 = kiwi.tokenize('ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µê³µì¥')
    res5 = kiwi.tokenize('ë†í˜‘ìš©ì¸ìœ¡ê°€ê³µê³µì¥')
    res6 = kiwi.tokenize('ë†í˜‘ìš© ì¸ìœ¡ ê°€ê³µ ê³µì¥')
    assert res1[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res2[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res3[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res4[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res5[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res6[0].form != 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    
    kiwi.space_tolerance = 1
    res2 = kiwi.tokenize('ë†í˜‘ìš©ì¸ìœ¡ ê°€ê³µ ê³µì¥')
    res3 = kiwi.tokenize('ë†í˜‘ìš© ì¸ìœ¡ ê°€ê³µ ê³µì¥')
    res4 = kiwi.tokenize('ë†í˜‘ìš© ì¸ìœ¡ ê°€ê³µê³µì¥')
    assert res2[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res3[0].form != 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res4[0].form != 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'

    kiwi.space_tolerance = 2
    res3 = kiwi.tokenize('ë†í˜‘ìš© ì¸ìœ¡ ê°€ê³µ ê³µì¥')
    res4 = kiwi.tokenize('ë†í˜‘ìš© ì¸ìœ¡ ê°€ê³µê³µì¥')
    assert res3[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res4[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'

    res5 = kiwi.tokenize('ë†í˜‘ìš©\nì¸ìœ¡ ê°€ê³µ\nê³µì¥ì—ì„œ')
    assert res5[0].form == 'ë†í˜‘ ìš©ì¸ ìœ¡ê°€ê³µ ê³µì¥'
    assert res5[0].line_number == 0
    assert res5[1].line_number == 2

def test_swtokenizer():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    print(tokenizer.vocab)
    print(tokenizer.config)
    strs = [
        "",
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.", 
        "ê°ì‚¬íˆ ë¨¹ê² ìŠµë‹ˆë‹¹!",
        "ë…¸ë˜ì§„ ì†í†±ì„ ë´¤ë˜ê±¸ìš”.",
        "ì œì„ìŠ¤ì›¹ìš°ì£¼ì²œì²´ë§ì›ê²½",
        "ê·¸ë§Œí•´ì—¬~",
    ]
    for s in strs:
        token_ids = tokenizer.encode(s)
        token_ids, offset = tokenizer.encode(s, return_offsets=True)
        decoded = tokenizer.decode(token_ids)
        assert s == decoded

def test_swtokenizer_batch():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    strs = [
        "",
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.", 
        "ê°ì‚¬íˆ ë¨¹ê² ìŠµë‹ˆë‹¹!",
        "ë…¸ë˜ì§„ ì†í†±ì„ ë´¤ë˜ê±¸ìš”.",
        "ì œì„ìŠ¤ì›¹ìš°ì£¼ì²œì²´ë§ì›ê²½",
        "ê·¸ë§Œí•´ì—¬~",
    ]
    for token_ids, s in zip(tokenizer.encode(strs), strs):
        decoded = tokenizer.decode(token_ids)
        assert s == decoded

def test_swtokenizer_morph():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    
    token_ids = tokenizer.encode("í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.")

    morphs = [
        ('í•œêµ­ì–´', 'NNP', False), 
        ('ì—', 'JKB',), 
        ('íŠ¹í™”', 'NNG', True), 
        ('ë˜', 'XSV',), 
        ('á†«', 'ETM', False), 
        ('í† í¬ë‚˜ì´ì €', 'NNG', True), 
        ('ì´', 'VCP', False), 
        ('á†¸ë‹ˆë‹¤', 'EF',), 
        ('.', 'SF', False),
    ]

    token_ids_from_morphs = tokenizer.encode_from_morphs(morphs)

    assert (token_ids == token_ids_from_morphs).all()

    token_ids_from_morphs, offsets = tokenizer.encode_from_morphs(morphs, return_offsets=True)

    assert offsets.tolist() == [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [5, 6], [5, 6], [6, 7], [7, 8], [8, 9]]

def test_swtokenizer_tokenize_encode():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/test/written.tokenizer.json')
    sents = [
        "í•œêµ­ì–´ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.",
        "í™ˆí˜ì´ì§€ëŠ” https://bab2min.github.io/kiwipiepy ì…ë‹ˆë‹¤."
    ]
    
    for sent in sents:
        ref_token_ids = tokenizer.encode(sent)
        ref_morphs = tokenizer.kiwi.tokenize(sent, normalize_coda=True, z_coda=True)
        morphs, token_ids, offset = tokenizer.tokenize_encode(sent, return_offsets=True)
        assert [m.tagged_form for m in morphs] == [m.tagged_form for m in ref_morphs]
        assert token_ids.tolist() == ref_token_ids.tolist()

    for (morphs, token_ids, offset), sent in zip(tokenizer.tokenize_encode(sents, return_offsets=True), sents):
        ref_token_ids = tokenizer.encode(sent)
        ref_morphs = tokenizer.kiwi.tokenize(sent, normalize_coda=True, z_coda=True)
        assert [m.tagged_form for m in morphs] == [m.tagged_form for m in ref_morphs]
        assert token_ids.tolist() == ref_token_ids.tolist()

def test_swtokenizer_offset():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/tokenizers/kor.32k.json')
    for sent in [
        'ğŸ”´ğŸŸ ğŸŸ¡ğŸŸ¢ğŸ”µğŸŸ£ğŸŸ¤âš«âšª\n'
    ]:
        token_ids, offsets = tokenizer.encode(sent, return_offsets=True)
        assert len(token_ids) == len(offsets)

def test_swtokenizer_morph_offset():
    tokenizer = sw_tokenizer.SwTokenizer('Kiwi/tokenizers/kor.32k.json')
    morphs = [
        ('ì¹¼ìŠ˜', 'NNG', True), 
        ('Â·', 'SP', False), 
        ('ë§ˆê·¸ë„¤ìŠ˜', 'NNG', False), 
        ('ë“±', 'NNB', True), 
        ('ì´', 'JKS', False), 
        ('ë§ì´', 'MAG', True), 
        ('í•¨ìœ ', 'NNG', True), 
        ('ë˜', 'XSV', False), 
        ('ì–´', 'EC', False), 
        ('ìˆ', 'VX', True), 
        ('ì–´', 'EC', False)
    ]
    token_ids, offsets = tokenizer.encode_from_morphs(morphs, return_offsets=True)
    assert len(token_ids) == len(offsets)
    assert offsets[2:7].tolist() == [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3]]

def test_swtokenizer_trainer_empty():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [], 
        config,
        4000,
    )

def test_swtokenizer_trainer_small():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        ["ì´ëŸ° ì €ëŸ° ë¬¸ì¥", "ì´ë ‡ê²Œ ì €ë ‡ê²Œ ì²˜ë¦¬í•´ì„œ", "ì´ë ‡ê³  ì €ë ‡ê³ ", "ì´ëŸ° ì €ëŸ° ê²°ê³¼ë¥¼", "ì–»ì—ˆë‹¤"], 
        config,
        4000,
    )

def test_swtokenizer_trainer_digits():
    kiwi = Kiwi(num_workers=0)
    config = sw_tokenizer.SwTokenizerConfig()

    tokenizer = sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [f"ë“œë””ì–´ ì œ{i}íšŒ í‰ê°€" for i in range(1, 1000)], 
        config,
        4000,
        kiwi=kiwi,
        prevent_mixed_digit_tokens=False,
    )
    mixed_digit = [k for k in tokenizer.vocab if re.search(r'ì œ[0-9]|[0-9]íšŒ', k)]
    assert len(mixed_digit) > 0

    tokenizer = sw_tokenizer.SwTokenizer.train(
        'test.json', 
        [f"ë“œë””ì–´ ì œ{i}íšŒ í‰ê°€" for i in range(1, 1000)], 
        config,
        4000,
        kiwi=kiwi,
        prevent_mixed_digit_tokens=True,
    )
    mixed_digit = [k for k in tokenizer.vocab if re.search(r'ì œ[0-9]|[0-9]íšŒ', k)]
    assert len(mixed_digit) == 0

def test_swtokenizer_trainer():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        'test.json', 
        itertools.chain.from_iterable(open(f, encoding='utf-8') for f in (
            'kiwipiepy/documentation.md', 
            'kiwipiepy/_wrap.py', 
        )), 
        config,
        4000,
    )

def test_swtokenizer_trainer_multiple_vocab_sizes():
    config = sw_tokenizer.SwTokenizerConfig()
    sw_tokenizer.SwTokenizer.train(
        ['test.json', 'test2.json', 'test3.json'], 
        itertools.chain.from_iterable(open(f, encoding='utf-8') for f in (
            'kiwipiepy/documentation.md', 
            'kiwipiepy/_wrap.py', 
        )), 
        config,
        [4000, 2000, 1000],
    )

def test_analyze_single():
    kiwi = Kiwi()
    for line in open(curpath + '/test_corpus/constitution.txt', encoding='utf-8'):
        toks, score = kiwi.analyze(line)[0]
    for t in toks:
        print(t.form, t.tag, t.start, t.end, t.len, t.id, t.base_form, t.base_id)
        break

def test_extract_words():
    kiwi = Kiwi()
    ret = kiwi.extract_words(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2)
    print(ret)


def test_tweet():
    kiwi = Kiwi()
    kiwi.analyze('''#ë°”ë‘‘#ì¥ê¸°#ì˜¤ëª© ê·€ìš”ë¯¸#ë³´ë“œíŒğŸ¥
#ì–´ë¦°ì´ì„ë¸”ë¦¬ì˜ ë†€ì´ì˜€ëŠ”ë°, ì´ì œëŠ” ê°€ë¬¼ê°¸ë¬¼ğŸ™„ëª¨ë¥´ê² 
ì¥ì´ìš”~ë©ì´ìš”~ã…ã…ã…ë‹¤ì‹œ í•œ ë²ˆ ì¬ë¯¸ë¥¼ ë¶™ì—¬ ë³´ê¹Œã…
í•  ì¼ì´ íƒœì‚°ì¸ë°ğŸ˜­, í•˜ê³  ì‹¶ì€ê±´ ë¬´ê¶ë¬´ì§„ğŸ¤¦â€â™€ï¸ í° ì¼ì´ë‹¤''')

def test_new_analyze_multi():
    kiwi = Kiwi()
    for res in kiwi.analyze(open(curpath + '/test_corpus/constitution.txt', encoding='utf-8')):
        pass

def test_bug_33():
    kiwi = Kiwi()
    kiwi.add_user_word('ê¹€ê°‘ê°‘', 'NNP')
    print(kiwi.analyze("ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘"))

def test_bug_38():
    text = "ì´ ì˜ˆìœ ê½ƒì€ ë…ì„ í’ˆì—ˆì§€ë§Œ ì§„ì§œ ì•„ë¦„ë‹¤ì›€ì„ ê°€ì§€ê³  ìˆì–´ìš”"
    kiwi = Kiwi(integrate_allomorph=True)
    print(kiwi.analyze(text))
    kiwi = Kiwi(integrate_allomorph=False)
    print(kiwi.analyze(text))

def test_property():
    kiwi = Kiwi()
    print(kiwi.integrate_allomorph)
    kiwi.integrate_allomorph = False
    print(kiwi.integrate_allomorph)
    print(kiwi.cutoff_threshold)
    kiwi.cutoff_threshold = 1
    print(kiwi.cutoff_threshold)

def test_stopwords():
    kiwi = Kiwi()
    tokens, _ = kiwi.analyze('ë¶ˆìš©ì–´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤ '
                             'ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ ìŸ¤ë„ ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ '
                             'ì§€ê¸ˆì€ 2021ë…„ 11ì›”ì´ë‹¤.')[0]
    stopwords = Stopwords()
    print(set(tokens) - set(stopwords.filter(tokens)))
    filename = curpath + '/test_corpus/custom_stopwords.txt'
    stopwords = Stopwords(filename)

    stopwords.add(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == True

    stopwords.remove(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == False
    print(set(tokens) - set(stopwords.filter(tokens)))

def test_tokenize():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    tokens = kiwi.tokenize(text, normalize_coda=True)
    print(tokens)

    tokens_by_sent = kiwi.tokenize(text, normalize_coda=True, split_sents=True)
    for tokens in tokens_by_sent:
        print(tokens)

def test_tokenize_with_stopwords():
    kiwi = Kiwi()
    stopwords = Stopwords()
    tokens = kiwi.tokenize("[^^ ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤.]", stopwords=stopwords)

    assert tokens[0].form == 'ê°•ì•„ì§€'
    assert tokens[1].form == 'ì¢‹ì•„í•˜'

def test_split_into_sents():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    sents = kiwi.split_into_sents(text, normalize_coda=True)
    assert len(sents) == 6

    assert sents[0].text == "ë‹¤ë…€ì˜¨ í›„ê¸°"
    assert sents[1].text == "<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.>"
    assert sents[2].text == "ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš”"
    assert sents[3].text == "ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã…"
    assert sents[4].text == "ê·¸ ë§›ì´ í¬ìœ¼.."
    assert sents[5].text == "ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"

def test_add_rule():
    kiwi = Kiwi(load_typo_dict=False)
    ores, oscore = kiwi.analyze("í–ˆì–´ìš”! í•˜ì–ì•„ìš”! í• ê¹Œìš”? ì¢‹ì•„ìš”!")[0]

    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=0)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©? ì¢‹ì•„ìš©!")[0]
    assert score == oscore

    kiwi = Kiwi(load_typo_dict=False)
    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=-1)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©? ì¢‹ì•„ìš©!")[0]
    assert abs(score - (oscore - 4)) < 1e-3

def test_add_pre_analyzed_word():
    kiwi = Kiwi()
    ores = kiwi.tokenize("íŒ…ê²¼ì–´")

    try:
        kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV"), "ì—ˆ/EP", "ì–´/EF"])
        raise AssertionError("expected to raise `ValueError`")
    except ValueError:
        pass
    except:
        raise

    kiwi.add_user_word("íŒ…ê¸°", "VV", orig_word="íŠ•ê¸°")
    kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV", 0, 2), ("ì—ˆ", "EP", 1, 2), ("ì–´", "EF", 2, 3)])

    res = kiwi.tokenize("íŒ…ê²¼ì–´...")

    assert res[0].form == "íŒ…ê¸°" and res[0].tag == "VV" and res[0].start == 0 and res[0].end == 2
    assert res[1].form == "ì—ˆ" and res[1].tag == "EP" and res[1].start == 1 and res[1].end == 2
    assert res[2].form == "ì–´" and res[2].tag == "EF" and res[2].start == 2 and res[2].end == 3
    assert res[3].form == "..." and res[3].tag == "SF" and res[3].start == 3 and res[3].end == 6

    kiwi.add_pre_analyzed_word('ê²©ìíŒ', [('ê²©ì', 'NNG'), ('íŒ','NNG')], 100)

    res = kiwi.tokenize("ë°”ë‘‘íŒ ëª¨ì–‘ì˜ ê²©ìíŒì„ í´")
    assert res[3].form == "ê²©ì"
    assert res[3].span == (8, 10)
    assert res[4].form == "íŒ"
    assert res[4].span == (10, 11)

def test_space_tolerance():
    kiwi = Kiwi()
    s = "ë„ ì–´ ì“° ê¸° ë¬¸ ì œ ê°€ ìˆ ìŠµ ë‹ˆ ë‹¤"
    kiwi.space_tolerance = 0
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 1
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 2
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 3
    print(kiwi.tokenize(s))

def test_space():
    kiwi = Kiwi()
    res0 = kiwi.space("ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.")
    assert res0 == "ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë„¤ ì´ê±¸ êµì •í•´ ì¤˜."

    res1 = kiwi.space(" ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ")
    assert res1 == " ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤, ì´ê±¸ êµì •í•´ ì¤˜. "

    res2 = kiwi.space("<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.")
    assert res2 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res3 = kiwi.space("<Kiwipiepy>ëŠ” í˜• íƒœ ì†Œ ë¶„ ì„ ê¸° ì´ ì— ìš”~ 0.11.0 ë²„ ì „ ì´ ë‚˜ ì™” ì–´ ìš” .", reset_whitespace=True)
    assert res3 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res_a = list(kiwi.space([
        "ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.",
        " ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ",
        "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.",
    ]))
    assert res_a == [res0, res1, res2]

def test_glue():
    chunks = """KorQuAD 2.0ì€ ì´ 100,000+ ìŒìœ¼ë¡œ êµ¬ì„±ëœ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì´ë‹¤. ê¸°ì¡´ ì§ˆì˜ì‘ë‹µ í‘œì¤€ ë°ì´
í„°ì¸ KorQuAD 1.0ê³¼ì˜ ì°¨ì´ì ì€ í¬ê²Œ ì„¸ê°€ì§€ê°€ ìˆëŠ”ë° ì²« ë²ˆì§¸ëŠ” ì£¼ì–´ì§€ëŠ” ì§€ë¬¸ì´ í•œë‘ ë¬¸ë‹¨ì´ ì•„ë‹Œ ìœ„
í‚¤ë°±ê³¼ í•œ í˜ì´ì§€ ì „ì²´ë¼ëŠ” ì ì´ë‹¤. ë‘ ë²ˆì§¸ë¡œ ì§€ë¬¸ì— í‘œì™€ ë¦¬ìŠ¤íŠ¸ë„ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— HTML tagë¡œ
êµ¬ì¡°í™”ëœ ë¬¸ì„œì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•˜ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹µë³€ì´ ë‹¨ì–´ í˜¹ì€ êµ¬ì˜ ë‹¨ìœ„ë¿ ì•„ë‹ˆë¼ ë¬¸ë‹¨, í‘œ, ë¦¬
ìŠ¤íŠ¸ ì „ì²´ë¥¼ í¬ê´„í•˜ëŠ” ê¸´ ì˜ì—­ì´ ë  ìˆ˜ ìˆë‹¤. Baseline ëª¨ë¸ë¡œ êµ¬ê¸€ì´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•œ BERT
Multilingualì„ í™œìš©í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼ F1 ìŠ¤ì½”ì–´ 46.0%ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ëŠ” ì‚¬ëŒì˜ F1 ì ìˆ˜
85.7%ì— ë¹„í•´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¡œ, ë³¸ ë°ì´í„°ê°€ ë„ì „ì ì¸ ê³¼ì œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë³¸ ë°ì´í„°ì˜ ê³µê°œë¥¼ í†µí•´
í‰ë¬¸ì— êµ­í•œë˜ì–´ ìˆë˜ ì§ˆì˜ì‘ë‹µì˜ ëŒ€ìƒì„ ë‹¤ì–‘í•œ ê¸¸ì´ì™€ í˜•ì‹ì„ ê°€ì§„ real world taskë¡œ í™•ì¥í•˜ê³ ì í•œë‹¤""".split('\n')
    
    kiwi = Kiwi()
    ret, space_insertions = kiwi.glue(chunks, return_space_insertions=True)
    assert space_insertions == [False, False, True, False, True, True, True]

def test_glue_empty():
    kiwi = Kiwi()
    kiwi.glue([])

def test_join():
    kiwi = Kiwi()
    tokens = kiwi.tokenize("ì´ë ‡ê²Œ í˜•íƒœì†Œë¡œ ë¶„í•´ëœ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹  ìˆ˜ ìˆì„ê¹Œìš”?")
    
    assert kiwi.join(tokens) == "ì´ë ‡ê²Œ í˜•íƒœì†Œë¡œ ë¶„í•´ëœ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹  ìˆ˜ ìˆì„ê¹Œìš”?"

    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV"), ("ì–´ìš”", "EF")]) 
        == "ì™œ ì €í•œí…Œ ë¬¼ì–´ìš”"
    )
    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV-R"), ("ì–´ìš”", "EF")])
        == "ì™œ ì €í•œí…Œ ë¬»ì–´ìš”"
    )
    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV-I"), ("ì–´ìš”", "EF")])
        == "ì™œ ì €í•œí…Œ ë¬¼ì–´ìš”"
    )

    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB", True), ("ë¬»", "VV-I"), ("ì–´ìš”", "EF")])
        == "ì™œ ì € í•œí…Œ ë¬¼ì–´ìš”"
    )

    assert (kiwi.join([("ì™œ", "MAG"), ("ì €", "NP"), ("í•œí…Œ", "JKB"), ("ë¬»", "VV-I", False), ("ì–´ìš”", "EF")])
        == "ì™œ ì €í•œí…Œë¬¼ì–´ìš”"
    )

def test_join_with_positions():
    kiwi = Kiwi()
    joined, positions = kiwi.join([('ğŸ¥', 'SW'), ('í•˜', 'VV'), ('ì—ˆ', 'EP'), ('ëŠ”ë°', 'EF')], return_positions=True)
    assert joined == 'ğŸ¥í–ˆëŠ”ë°'
    assert positions == [(0, 1), (1, 2), (1, 2), (2, 4)]

def test_join_edge_cases():
    kiwi = Kiwi()
    for c in [
        'ê°€ê²©ì´ ì‹¼ ê²ƒì´ ì´ê²ƒë¿ì´ì—ìš”.'
    ]:
        tokens = kiwi.tokenize(c)
        restored = kiwi.join(tokens)
        raw = kiwi.join([(t.form, t.tag) for t in tokens])
        assert c == restored
        assert c == raw

def test_bug_87():
    text = "í•œê¸€(éŸ“ã[1], ì˜ì–´: Hangeul[2]ë˜ëŠ” Hangul[3])ì€ í•œêµ­ì–´ì˜ ê³µì‹ë¬¸ìë¡œì„œ, ì„¸ì¢…ì´ í•œêµ­ì–´ë¥¼ í‘œê¸°í•˜ê¸° ìœ„í•˜ì—¬ ì°½ì œí•œ ë¬¸ìì¸ 'í›ˆë¯¼ì •ìŒ'(è¨“æ°‘æ­£éŸ³)ì„ 20ì„¸ê¸° ì´ˆë°˜ ì´í›„ ë‹¬ë¦¬ ì´ë¥´ëŠ” ëª…ì¹­ì´ë‹¤.[4][5] í•œê¸€ì´ë€ ì´ë¦„ì€ ì£¼ì‹œê²½ ì„ ìƒê³¼ êµ­ì–´ì—°êµ¬í•™íšŒ íšŒì›ë“¤ì— ì˜í•´ ì§€ì–´ì§„ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë©°[6][7][8][9] ê·¸ ëœ»ì€ 'ìœ¼ëœ¸ì´ ë˜ëŠ” í°ê¸€', 'ì˜¤ì§ í•˜ë‚˜ë¿ì¸ í°ê¸€', 'í•œêµ­ì¸ì˜ ê¸€ì'ì´ë‹¤.[6][10] í•œê¸€ì˜ ë˜ ë‹¤ë¥¸ ë³„ì¹­ìœ¼ë¡œëŠ” ì •ìŒ(æ­£éŸ³), ì–¸ë¬¸(è«ºæ–‡)[11], ì–¸ì„œ(è«ºæ›¸), ë°˜ì ˆ(ååˆ‡), ì•”í´, ì•„í–‡ê¸€, ê°€ê°¸ê¸€, êµ­ë¬¸(åœ‹æ–‡)[12] ë“±ì´ ìˆë‹¤.[5]"
    kiwi = Kiwi()
    print(kiwi.join(kiwi.tokenize(text)))

def test_typo_transformer():
    print(basic_typos.generate("ì•ˆë¼"))

def test_typo_correction():
    if sys.maxsize <= 2**32:
        print("[skipped this test in 32bit OS.]", file=sys.stderr)
        return
    kiwi = Kiwi(typos='basic')
    ret = kiwi.tokenize("ì™¸ì•Šë€ëŒ€?")
    assert ret[0].form == 'ì™œ'
    assert ret[1].form == 'ì•ˆ'
    assert ret[2].form == 'ë˜'
    assert ret[3].form == 'á†«ëŒ€'
    print(ret)

def test_continual_typo():
    kiwi = Kiwi(typos='continual')
    tokens = kiwi.tokenize('ì˜¤ëŠ˜ì‚¬ë¬´ì‹œë ˆì„œ')
    assert tokens[1].form == 'ì‚¬ë¬´ì‹¤'
    assert tokens[2].form == 'ì—ì„œ'

    tokens = kiwi.tokenize('ì§€ê°€ìº¤ì–´ìš”')
    assert tokens[0].form == 'ì§€ê°'
    assert tokens[1].form == 'í•˜'

    kiwi = Kiwi(typos='basic_with_continual')
    tokens = kiwi.tokenize('ì›¨ ì§€ê°€ìº¤ë‹ˆ?')
    assert tokens[0].form == 'ì™œ'
    assert tokens[1].form == 'ì§€ê°'
    assert tokens[2].form == 'í•˜'

def test_sbg():
    kiwi = Kiwi(model_type='knlm')
    print(kiwi.tokenize('ì´ ë²ˆí˜¸ë¡œ ì „í™”ë¥¼ ì´ë”°ê°€ ê¼­ ë°˜ë“œì‹œ ê±¸ì–´.'))
    kiwi = Kiwi(model_type='sbg')
    print(kiwi.tokenize('ì´ ë²ˆí˜¸ë¡œ ì „í™”ë¥¼ ì´ë”°ê°€ ê¼­ ë°˜ë“œì‹œ ê±¸ì–´.'))

def test_issue_92():
    if sys.maxsize <= 2**32:
        print("[skipped this test in 32bit OS.]", file=sys.stderr)
        return
    kiwi = Kiwi(typos='basic')
    try:
        kiwi.join(kiwi.analyze('ì˜'))
        raise AssertionError("expected to raise `ValueError`")
    except ValueError:
        pass
    except:
        raise

    kiwi.join([('ì‚¬ë‘','NNG')])

def test_unicode():
    kiwi = Kiwi()
    print(repr(kiwi.tokenize("ê²° ë¡  189   ì°¸ê³ ë¬¸í—Œ 191   ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ó° ")))

def test_template():
    kiwi = Kiwi()
    tpl = kiwi.template("{}ê°€ {}ì„ {}ì—ˆë‹¤.")

    res = tpl.format(("ë‚˜", "NP"), ("ê³µë¶€", "NNG"), ("í•˜", "VV"))
    assert res == "ë‚´ê°€ ê³µë¶€ë¥¼ í–ˆë‹¤."

    res = tpl.format(("ë„ˆ", "NP"), ("ë°¥", "NNG"), ("ë¨¹", "VV"))
    assert res == "ë„¤ê°€ ë°¥ì„ ë¨¹ì—ˆë‹¤."

    res = tpl.format(("ìš°ë¦¬", "NP"), ("ê¸¸", "NNG"), ("ë¬»", "VV-I"))
    assert res == "ìš°ë¦¬ê°€ ê¸¸ì„ ë¬¼ì—ˆë‹¤."

    res = tpl.format(5, "str", {"dict":"dict"})
    assert res == "5ê°€ strë¥¼ {'dict': 'dict'}ì—ˆë‹¤."

    tpl = kiwi.template("{:.5f}ê°€ {!r}ì„ {}ì—ˆë‹¤.")
    
    res = tpl.format(5, "str", {"dict":"dict"})
    assert res == "5.00000ê°€ 'str'ë¥¼ {'dict': 'dict'}ì—ˆë‹¤."

    caught_error = None
    try:
        res = tpl.format(("ìš°ë¦¬", "NP"), ("ê¸¸", "NNG"), ("ë¬»", "VV-I"))
    except ValueError:
        caught_error = True
    assert caught_error

    res = tpl.format(5, ("ê¸¸", "NNG"), ("ë¬»", "VV-I"))
    assert res == "5.00000ê°€ ('ê¸¸', 'NNG')ë¥¼ ë¬¼ì—ˆë‹¤."

    tpl = kiwi.template("{}ê°€ {}ë¥¼ {}\ã„´ë‹¤.")
    res = tpl.format([("ìš°ë¦¬", "NP"), ("ë“¤", "XSN")], ("ê¸¸", "NNG"), ("ë¬»", "VV-I"))
    assert res == "ìš°ë¦¬ë“¤ì´ ê¸¸ì„ ë¬»ëŠ”ë‹¤."

def test_issue_145():
    kiwi = Kiwi()
    stopwords = Stopwords()
    kiwi.add_user_word('íŒ”ì´', 'XSV', 10)
    text = "ë£¨ì‰°(ë…¸ì‹ )ì˜ ã€Œì•„Qì •ì „ã€ì€ ì£¼ì¸ê³µì„ í†µí•´ ì¤‘êµ­ë¯¼ì¡±ì˜ ë³‘í,ë…¸ì˜ˆê·¼ì„±ì„ ê¸°íƒ„ì—†ì´ ì§€ì í•œ ì‘í’ˆì´ë‹¤. ë‚ í’ˆíŒ”ì´ë¥¼ í•˜ë©° ê·¸ëŸ­ì €ëŸ­ ì‚´ì•„ê°€ëŠ” ë– ëŒì´ ë†ë¯¼ ì•„QëŠ” ìê¸°ë„ ëª¨ë¥´ëŠ” ì‚¬ì´ì— í˜ëª…ì˜ ì™€ì¤‘ì— íœ˜ë§ë ¤ ë°˜ë€ì£„ë¡œ ì²´í¬ë˜ê³  ì‚¬í˜•ì„ ê³ ë¥¼ ë°›ëŠ”ë‹¤.ê¹Œë‹­ë„ ëª¨ë¥´ê³  ì‚¬í˜•ì§‘í–‰ ì„œë¥˜ì— ì„œëª…ì„ í•˜ê²Œ ë˜ì§€ë§Œ ê¸€ìë¥¼ ì“¸ì¤„ ëª¨ë¥´ëŠ” ì¼ìë¬´ì‹ ì•„QëŠ” ì˜¨ í˜ì„ ê¸°ìš¸ì—¬ ë™ê·¸ë¼ë¯¸ë¥¼ ê²¨ìš° ê·¸ë¦° í›„.."
    tokens = kiwi.tokenize(text, split_sents= True, stopwords = stopwords, blocklist = ['ê»ŒíŒ”ì´/NNG','í’ˆíŒ”ì´/NNG', 'ë‚ í’ˆíŒ”ì´/NNG'])
    assert tokens
