import os

from kiwipiepy import Kiwi
from kiwipiepy.utils import Stopwords

curpath = os.path.dirname(os.path.abspath(__file__))

class FileReader:
    def __init__(self, path):
        self.path = path

    def __iter__(self):
        yield from open(self.path, encoding='utf-8')

def test_analyze_single():
    kiwi = Kiwi()
    for line in open(curpath + '/test_corpus/constitution.txt', encoding='utf-8'):
        toks, score = kiwi.analyze(line)[0]
    for t in toks:
        print(t.form, t.tag, t.start, t.end, t.len, t.id, t.base_form, t.base_id)
        break

def test_extract_words():
    kiwi = Kiwi()
    ret = kiwi.extract_words(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2)
    print(ret)

def test_perform():
    kiwi = Kiwi()
    for res in kiwi.perform(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2):
        print(res)
        break

def test_tweet():
    kiwi = Kiwi()
    kiwi.analyze('''#ë°”ë‘‘#ì¥ê¸°#ì˜¤ëª© ê·€ìš”ë¯¸#ë³´ë“œíŒğŸ¥
#ì–´ë¦°ì´ì„ë¸”ë¦¬ì˜ ë†€ì´ì˜€ëŠ”ë°, ì´ì œëŠ” ê°€ë¬¼ê°¸ë¬¼ğŸ™„ëª¨ë¥´ê² 
ì¥ì´ìš”~ë©ì´ìš”~ã…ã…ã…ë‹¤ì‹œ í•œ ë²ˆ ì¬ë¯¸ë¥¼ ë¶™ì—¬ ë³´ê¹Œã…
í•  ì¼ì´ íƒœì‚°ì¸ë°ğŸ˜­, í•˜ê³  ì‹¶ì€ê±´ ë¬´ê¶ë¬´ì§„ğŸ¤¦â€â™€ï¸ í° ì¼ì´ë‹¤''')

def test_new_analyze_multi():
    kiwi = Kiwi()
    for res in kiwi.analyze(open(curpath + '/test_corpus/constitution.txt', encoding='utf-8')):
        pass

def test_bug_33():
    kiwi = Kiwi()
    kiwi.add_user_word('ê¹€ê°‘ê°‘', 'NNP')
    print(kiwi.analyze("ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘"))

def test_bug_38():
    text = "ì´ ì˜ˆìœ ê½ƒì€ ë…ì„ í’ˆì—ˆì§€ë§Œ ì§„ì§œ ì•„ë¦„ë‹¤ì›€ì„ ê°€ì§€ê³  ìˆì–´ìš”"
    kiwi = Kiwi(integrate_allomorph=True)
    print(kiwi.analyze(text))
    kiwi = Kiwi(integrate_allomorph=False)
    print(kiwi.analyze(text))

def test_property():
    kiwi = Kiwi()
    print(kiwi.integrate_allomorph)
    kiwi.integrate_allomorph = False
    print(kiwi.integrate_allomorph)
    print(kiwi.cutoff_threshold)
    kiwi.cutoff_threshold = 1
    print(kiwi.cutoff_threshold)

def test_stopwords():
    kiwi = Kiwi()
    tokens, _ = kiwi.analyze('ë¶ˆìš©ì–´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤ '
                             'ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ ìŸ¤ë„ ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ '
                             'ì§€ê¸ˆì€ 2021ë…„ 11ì›”ì´ë‹¤.')[0]
    stopwords = Stopwords()
    print(set(tokens) - set(stopwords.filter(tokens)))
    filename = curpath + '/test_corpus/custom_stopwords.txt'
    stopwords = Stopwords(filename)

    stopwords.add(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == True

    stopwords.remove(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == False
    print(set(tokens) - set(stopwords.filter(tokens)))

def test_tokenize():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    tokens = kiwi.tokenize(text, normalize_coda=True)
    print(tokens)

    tokens_by_sent = kiwi.tokenize(text, normalize_coda=True, split_sents=True)
    for tokens in tokens_by_sent:
        print(tokens)

def test_tokenize_with_stopwords():
    kiwi = Kiwi()
    stopwords = Stopwords()
    tokens = kiwi.tokenize("[^^ ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤.]", stopwords=stopwords)

    assert tokens[0].form == 'ê°•ì•„ì§€'
    assert tokens[1].form == 'ì¢‹ì•„í•˜'

def test_split_into_sents():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    sents = kiwi.split_into_sents(text, normalize_coda=True)
    assert len(sents) == 6

    assert sents[0].text == "ë‹¤ë…€ì˜¨ í›„ê¸°"
    assert sents[1].text == "<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.>"
    assert sents[2].text == "ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš”"
    assert sents[3].text == "ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã…"
    assert sents[4].text == "ê·¸ ë§›ì´ í¬ìœ¼.."
    assert sents[5].text == "ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"

def test_add_rule():
    kiwi = Kiwi()
    ores, oscore = kiwi.analyze("í–ˆì–´ìš”! í•˜ì–ì•„ìš”! í• ê¹Œìš”?")[0]

    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=0)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©?")[0]
    assert score == oscore

    kiwi = Kiwi()
    assert len(kiwi.add_re_rule("EF", r"ìš”$", "ìš©", score=-1)) > 0
    res, score = kiwi.analyze("í–ˆì–´ìš©! í•˜ì–ì•„ìš©! í• ê¹Œìš©?")[0]
    assert score == oscore - 3

def test_add_pre_analyzed_word():
    kiwi = Kiwi()
    ores = kiwi.tokenize("íŒ…ê²¼ì–´")

    try:
        kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV"), "ì—ˆ/EP", "ì–´/EF"])
        raise AssertionError("expected to raise `ValueError`")
    except ValueError:
        pass
    except:
        raise AssertionError("expected to raise `ValueError`")

    kiwi.add_user_word("íŒ…ê¸°", "VV", orig_word="íŠ•ê¸°")
    kiwi.add_pre_analyzed_word("íŒ…ê²¼ì–´", [("íŒ…ê¸°", "VV", 0, 2), ("ì—ˆ", "EP", 1, 2), ("ì–´", "EF", 2, 3)])

    res = kiwi.tokenize("íŒ…ê²¼ì–´...")

    assert res[0].form == "íŒ…ê¸°" and res[0].tag == "VV" and res[0].start == 0 and res[0].end == 2
    assert res[1].form == "ì—ˆ" and res[1].tag == "EP" and res[1].start == 1 and res[1].end == 2
    assert res[2].form == "ì–´" and res[2].tag == "EF" and res[2].start == 2 and res[2].end == 3
    assert res[3].form == "..." and res[3].tag == "SF" and res[3].start == 3 and res[3].end == 6

def test_space_tolerance():
    kiwi = Kiwi()
    s = "ë„ ì–´ ì“° ê¸° ë¬¸ ì œ ê°€ ìˆ ìŠµ ë‹ˆ ë‹¤"
    kiwi.space_tolerance = 0
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 1
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 2
    print(kiwi.tokenize(s))
    kiwi.space_tolerance = 3
    print(kiwi.tokenize(s))

def test_space():
    kiwi = Kiwi()
    res0 = kiwi.space("ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.")
    assert res0 == "ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë„¤ ì´ê±¸ êµì •í•´ ì¤˜."

    res1 = kiwi.space(" ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ")
    assert res1 == " ë„ì–´ì“°ê¸° ì—†ì´ ì‘ì„±ëœ í…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤, ì´ê±¸ êµì •í•´ ì¤˜. "

    res2 = kiwi.space("<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.")
    assert res2 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res3 = kiwi.space("<Kiwipiepy>ëŠ” í˜• íƒœ ì†Œ ë¶„ ì„ ê¸° ì´ ì— ìš”~ 0.11.0 ë²„ ì „ ì´ ë‚˜ ì™” ì–´ ìš” .", reset_whitespace=True)
    assert res3 == "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”."

    res_a = list(kiwi.space([
        "ë„ì–´ì“°ê¸°ì—†ì´ì‘ì„±ëœí…ìŠ¤íŠ¸ë„¤ì´ê±¸êµì •í•´ì¤˜.",
        " ë„ì–´ì“°ê¸°ì—†ì´ ì‘ì„±ëœí…ìŠ¤íŠ¸(http://github.com/bab2min/kiwipiepy )ë„¤,ì´ê±¸êµì •í•´ì¤˜. ",
        "<Kiwipiepy>ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì´ì—ìš”~ 0.11.0 ë²„ì „ì´ ë‚˜ì™”ì–´ìš”.",
    ]))
    assert res_a == [res0, res1, res2]


def test_glue():
    chunks = """KorQuAD 2.0ì€ ì´ 100,000+ ìŒìœ¼ë¡œ êµ¬ì„±ëœ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì´ë‹¤. ê¸°ì¡´ ì§ˆì˜ì‘ë‹µ í‘œì¤€ ë°ì´
í„°ì¸ KorQuAD 1.0ê³¼ì˜ ì°¨ì´ì ì€ í¬ê²Œ ì„¸ê°€ì§€ê°€ ìˆëŠ”ë° ì²« ë²ˆì§¸ëŠ” ì£¼ì–´ì§€ëŠ” ì§€ë¬¸ì´ í•œë‘ ë¬¸ë‹¨ì´ ì•„ë‹Œ ìœ„
í‚¤ë°±ê³¼ í•œ í˜ì´ì§€ ì „ì²´ë¼ëŠ” ì ì´ë‹¤. ë‘ ë²ˆì§¸ë¡œ ì§€ë¬¸ì— í‘œì™€ ë¦¬ìŠ¤íŠ¸ë„ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— HTML tagë¡œ
êµ¬ì¡°í™”ëœ ë¬¸ì„œì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•˜ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹µë³€ì´ ë‹¨ì–´ í˜¹ì€ êµ¬ì˜ ë‹¨ìœ„ë¿ ì•„ë‹ˆë¼ ë¬¸ë‹¨, í‘œ, ë¦¬
ìŠ¤íŠ¸ ì „ì²´ë¥¼ í¬ê´„í•˜ëŠ” ê¸´ ì˜ì—­ì´ ë  ìˆ˜ ìˆë‹¤. Baseline ëª¨ë¸ë¡œ êµ¬ê¸€ì´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•œ BERT
Multilingualì„ í™œìš©í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼ F1 ìŠ¤ì½”ì–´ 46.0%ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ëŠ” ì‚¬ëŒì˜ F1 ì ìˆ˜
85.7%ì— ë¹„í•´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¡œ, ë³¸ ë°ì´í„°ê°€ ë„ì „ì ì¸ ê³¼ì œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë³¸ ë°ì´í„°ì˜ ê³µê°œë¥¼ í†µí•´
í‰ë¬¸ì— êµ­í•œë˜ì–´ ìˆë˜ ì§ˆì˜ì‘ë‹µì˜ ëŒ€ìƒì„ ë‹¤ì–‘í•œ ê¸¸ì´ì™€ í˜•ì‹ì„ ê°€ì§„ real world taskë¡œ í™•ì¥í•˜ê³ ì í•œë‹¤""".split('\n')
    
    kiwi = Kiwi()
    ret, space_insertions = kiwi.glue(chunks, return_space_insertions=True)
    assert space_insertions == [False, False, True, False, True, True, True]
