import os
curpath = os.path.dirname(os.path.abspath(__file__))

from kiwipiepy import Kiwi
from kiwipiepy.utils import Stopwords

def test_analyze_single():
    kiwi = Kiwi()
    for line in open(curpath + '/test_corpus/constitution.txt', encoding='utf-8'):
        toks, score = kiwi.analyze(line)[0]
    for t in toks:
        print(t.form, t.tag, t.start, t.end, t.len)
        break


class FileReader:
    def __init__(self, path):
        self.path = path

    def __iter__(self):
        yield from open(self.path, encoding='utf-8')

def test_extract_words():
    kiwi = Kiwi()
    ret = kiwi.extract_words(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2)
    print(ret)

def test_perform():
    kiwi = Kiwi()
    for res in kiwi.perform(FileReader(curpath + '/test_corpus/constitution.txt'), min_cnt=2):
        print(res)
        break

def test_tweet():
    kiwi = Kiwi()
    kiwi.analyze('''#ë°”ë‘‘#ì¥ê¸°#ì˜¤ëª© ê·€ìš”ë¯¸#ë³´ë“œíŒğŸ¥
#ì–´ë¦°ì´ì„ë¸”ë¦¬ì˜ ë†€ì´ì˜€ëŠ”ë°, ì´ì œëŠ” ê°€ë¬¼ê°¸ë¬¼ğŸ™„ëª¨ë¥´ê² 
ì¥ì´ìš”~ë©ì´ìš”~ã…ã…ã…ë‹¤ì‹œ í•œ ë²ˆ ì¬ë¯¸ë¥¼ ë¶™ì—¬ ë³´ê¹Œã…
í•  ì¼ì´ íƒœì‚°ì¸ë°ğŸ˜­, í•˜ê³  ì‹¶ì€ê±´ ë¬´ê¶ë¬´ì§„ğŸ¤¦â€â™€ï¸ í° ì¼ì´ë‹¤''')

def test_new_analyze_multi():
    kiwi = Kiwi()
    for res in kiwi.analyze(open(curpath + '/test_corpus/constitution.txt', encoding='utf-8')):
        pass

def test_bug_33():
    kiwi = Kiwi()
    kiwi.add_user_word('ê¹€ê°‘ê°‘', 'NNP')
    print(kiwi.analyze("ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘ ê¹€ê°‘ê°‘"))

def test_bug_38():
    text = "ì´ ì˜ˆìœ ê½ƒì€ ë…ì„ í’ˆì—ˆì§€ë§Œ ì§„ì§œ ì•„ë¦„ë‹¤ì›€ì„ ê°€ì§€ê³  ìˆì–´ìš”"
    kiwi = Kiwi(integrate_allomorph=True)
    print(kiwi.analyze(text))
    kiwi = Kiwi(integrate_allomorph=False)
    print(kiwi.analyze(text))

def test_property():
    kiwi = Kiwi()
    print(kiwi.integrate_allomorph)
    kiwi.integrate_allomorph = False
    print(kiwi.integrate_allomorph)
    print(kiwi.cutoff_threshold)
    kiwi.cutoff_threshold = 1
    print(kiwi.cutoff_threshold)

def test_stopwords():
    kiwi = Kiwi()
    tokens, _ = kiwi.analyze('ë¶ˆìš©ì–´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤ '
                             'ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ ìŸ¤ë„ ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤ '
                             'ì§€ê¸ˆì€ 2021ë…„ 11ì›”ì´ë‹¤.')[0]
    stopwords = Stopwords()
    print(set(tokens) - set(stopwords.filter(tokens)))
    filename = curpath + '/test_corpus/custom_stopwords.txt'
    stopwords = Stopwords(filename)

    stopwords.add(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == True

    stopwords.remove(('ê°•ì•„ì§€', 'NNP'))
    assert (('ê°•ì•„ì§€', 'NNP') in stopwords) == False
    print(set(tokens) - set(stopwords.filter(tokens)))

def test_tokenize():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    tokens = kiwi.tokenize(text, normalize_coda=True)
    print(tokens)

    tokens_by_sent = kiwi.tokenize(text, normalize_coda=True, split_sents=True)
    for tokens in tokens_by_sent:
        print(tokens)

def test_tokenize_with_stopwords():
    kiwi = Kiwi()
    stopwords = Stopwords()
    tokens = kiwi.tokenize("[^^ ìš°ë¦¬ëŠ” ê°•ì•„ì§€ë¥¼ ì¢‹ì•„í•œë‹¤.]", stopwords=stopwords)

    assert tokens[0].form == 'ê°•ì•„ì§€'
    assert tokens[1].form == 'ì¢‹ì•„í•˜'

def test_split_into_sents():
    kiwi = Kiwi()
    text = "ë‹¤ë…€ì˜¨ í›„ê¸°\n\n<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.> ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš” ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã… ê·¸ ë§›ì´ í¬ìœ¼.. ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
    sents = kiwi.split_into_sents(text, normalize_coda=True)
    assert len(sents) == 6

    assert sents[0].text == "ë‹¤ë…€ì˜¨ í›„ê¸°"
    assert sents[1].text == "<ê°•ë‚¨ í† ë¼ì •ì— ë‹¤ë…€ì™”ìŠµë‹ˆë‹¤.>"
    assert sents[2].text == "ìŒì‹ë„ ë§›ìˆì—ˆì–´ìš”"
    assert sents[3].text == "ë‹¤ë§Œ ì—­ì‹œ í† ë¼ì • ë³¸ì  ë‹µì£ ?ã…ã……ã…"
    assert sents[4].text == "ê·¸ ë§›ì´ í¬ìœ¼.."
    assert sents[5].text == "ì•„ì£¼ ë§›ìˆì—ˆìŒ...! ^^"
